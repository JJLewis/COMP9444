{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as tnn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as topti\n",
    "from torchtext import data\n",
    "from torchtext.vocab import GloVe\n",
    "from imdb_dataloader import IMDB\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing():\n",
    "    def pre(x):\n",
    "        \"\"\"Called after tokenization\"\"\"\n",
    "        #print('pre')\n",
    "        #print(x)\n",
    "        return x\n",
    "\n",
    "    def post(batch, vocab):\n",
    "        \"\"\"Called after numericalization but prior to vectorization\"\"\"\n",
    "        #print(batch)\n",
    "        #print(vocab)\n",
    "        return batch\n",
    "\n",
    "    text_field = data.Field(lower=True, include_lengths=True, batch_first=True, preprocessing=pre, postprocessing=post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFunc():\n",
    "    \"\"\"\n",
    "    Define a loss function appropriate for the above networks that will\n",
    "    add a sigmoid to the output and calculate the binary cross-entropy.\n",
    "    \"\"\"\n",
    "    return tnn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "before labelField\n",
      "Train and dev\n",
      "<class 'imdb_dataloader.IMDB'>\n",
      "25000\n",
      "['for', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem.', 'imagine', 'a', 'movie', 'where', 'joe', 'piscopo', 'is', 'actually', 'funny!', 'maureen', 'stapleton', 'is', 'a', 'scene', 'stealer.', 'the', 'moroni', 'character', 'is', 'an', 'absolute', 'scream.', 'watch', 'for', 'alan', '\"the', 'skipper\"', 'hale', 'jr.', 'as', 'a', 'police', 'sgt.']\n",
      "<class 'list'>\n",
      "52\n",
      "Build Vocab\n",
      "Loaders\n"
     ]
    }
   ],
   "source": [
    "# Use a GPU if available, as it should be faster.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \" + str(device))\n",
    "\n",
    "# Load the training dataset, and create a data loader to generate a batch.\n",
    "textField = PreProcessing.text_field\n",
    "print('before labelField')\n",
    "labelField = data.Field(sequential=False)\n",
    "\n",
    "print(\"Train and dev\")\n",
    "train, dev = IMDB.splits(textField, labelField, train=\"train\", validation=\"dev\")\n",
    "print(type(train))\n",
    "print(len(train))\n",
    "print(train[0].text)\n",
    "print(type(train[0].text))\n",
    "print(len(train[0].text))\n",
    "\n",
    "print(\"Build Vocab\")\n",
    "textField.build_vocab(train, dev, vectors=GloVe(name=\"6B\", dim=50))\n",
    "labelField.build_vocab(train, dev)\n",
    "\n",
    "#return\n",
    "print(\"Loaders\")\n",
    "trainLoader, testLoader = data.BucketIterator.splits((train, dev), shuffle=True, batch_size=64,\n",
    "                                                     sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
    "# already batched up by here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Going to train\")\n",
    "    net = Network().to(device)\n",
    "    criterion =lossFunc()\n",
    "    optimiser = topti.Adam(net.parameters(), lr=0.001)  # Minimise the loss using the Adam algorithm.\n",
    "\n",
    "    for epoch in range(10):\n",
    "        running_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(trainLoader):\n",
    "            # Get a batch and potentially send it to GPU memory.\n",
    "            inputs, length, labels = textField.vocab.vectors[batch.text[0]].to(device), batch.text[1].to(\n",
    "                device), batch.label.type(torch.FloatTensor).to(device)\n",
    "\n",
    "            labels -= 1\n",
    "\n",
    "            # PyTorch calculates gradients by accumulating contributions to them (useful for\n",
    "            # RNNs).  Hence we must manually set them to zero before calculating them.\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            # Forward pass through the network.\n",
    "            output = net(inputs, length)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            # Calculate gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Minimise the loss according to the gradient.\n",
    "            optimiser.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 32 == 31:\n",
    "                print(\"Epoch: %2d, Batch: %4d, Loss: %.3f\" % (epoch + 1, i + 1, running_loss / 32))\n",
    "                running_loss = 0\n",
    "\n",
    "    num_correct = 0\n",
    "\n",
    "    # Save mode\n",
    "    torch.save(net.state_dict(), \"./model_notebook.pth\")\n",
    "    print(\"Saved model\")\n",
    "\n",
    "    # Evaluate network on the test dataset.  We aren't calculating gradients, so disable autograd to speed up\n",
    "    # computations and reduce memory usage.\n",
    "    with torch.no_grad():\n",
    "        for batch in testLoader:\n",
    "            # Get a batch and potentially send it to GPU memory.\n",
    "            inputs, length, labels = textField.vocab.vectors[batch.text[0]].to(device), batch.text[1].to(\n",
    "                device), batch.label.type(torch.FloatTensor).to(device)\n",
    "\n",
    "            labels -= 1\n",
    "\n",
    "            # Get predictions\n",
    "            outputs = torch.sigmoid(net(inputs, length))\n",
    "            predicted = torch.round(outputs)\n",
    "\n",
    "            num_correct += torch.sum(labels == predicted).item()\n",
    "\n",
    "    accuracy = 100 * num_correct / len(dev)\n",
    "\n",
    "    print(f\"Classification accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "before labelField\n",
      "Train and dev\n"
     ]
    }
   ],
   "source": [
    "# Use a GPU if available, as it should be faster.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \" + str(device))\n",
    "\n",
    "# Load the training dataset, and create a data loader to generate a batch.\n",
    "textField = PreProcessing.text_field\n",
    "print('before labelField')\n",
    "labelField = data.Field(sequential=False)\n",
    "\n",
    "print(\"Train and dev\")\n",
    "train, dev = IMDB.splits(textField, labelField, train=\"train\", validation=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropsies_d(sentence, min_drops, max_drops, keep_half=False):\n",
    "    if len(sentence) == 1:\n",
    "        return sentence\n",
    "    safe_max = max_drops if len(sentence)/(keep_half+1) > max_drops else len(sentence)/(keep_half+1)-(not keep_half)\n",
    "    num_drops = random.randint(min_drops, safe_max)\n",
    "    new_sentence = sentence.copy()\n",
    "    for _ in range(num_drops):\n",
    "        del new_sentence[random.randint(0, len(new_sentence)-1)]\n",
    "    \n",
    "    return new_sentence\n",
    "\n",
    "def dropsies_nd(sentence, prob, keep_half=False):\n",
    "    if len(sentence) == 1:\n",
    "        return sentence\n",
    "    \n",
    "    new_sentence = []\n",
    "    for word in sentence:\n",
    "        r = random.uniform(0,1)\n",
    "        if r > prob:\n",
    "            new_sentence.append(word)\n",
    "    \n",
    "    # if no words left, return a random number of words\n",
    "    if len(new_sentence) == 0 or (keep_half and len(new_sentence) < len(sentence)/2):\n",
    "        new_sentence = sentence.copy()\n",
    "        for _ in range(random.randint(1,len(new_sentence)/(keep_half+1)-(not keep_half))):\n",
    "            del new_sentence[random.randint(0,len(new_sentence)-1)]\n",
    "    \n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swapsie(sentence, to_swap_idx, max_dist):\n",
    "    '''\n",
    "    Single inplace swap\n",
    "    '''\n",
    "    distance = random.randint(1, max_dist)\n",
    "    direction = -1 if random.randint(0,1) == 0 else 1\n",
    "    other = to_swap_idx + (distance * direction)\n",
    "    other = other if other in range(0, len(sentence)) else [0,0,len(sentence)-1][direction+1]\n",
    "    tmp = sentence[to_swap_idx]\n",
    "    sentence[to_swap_idx] = sentence[other]\n",
    "    sentence[other] = tmp\n",
    "    \n",
    "def swapsies_nd(sentence, prob, max_dist):\n",
    "    new_sentence = sentence.copy()\n",
    "    did_swap = False\n",
    "    for i in range(0, len(new_sentence)):\n",
    "        r = random.uniform(0,1)\n",
    "        if r < prob:\n",
    "            did_swap = True\n",
    "            swapsie(new_sentence, i, max_dist)\n",
    "    if not did_swap:\n",
    "        swapsie(new_sentence, random.randint(0,len(sentence)-1), max_dist)\n",
    "    return new_sentence\n",
    "\n",
    "def swapsies_d(sentence, min_swaps, max_swaps, max_dist):\n",
    "    new_sentence = sentence.copy()\n",
    "    num_swaps = round(random.uniform(min_swaps, max_swaps))\n",
    "    print(num_swaps)\n",
    "    for _ in range(num_swaps):\n",
    "        to_swap_idx = random.randint(0, len(new_sentence)-1)\n",
    "        swapsie(new_sentence, to_swap_idx, max_dist)\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentie_nd(sentence, drop_prob, swap_prob, max_dist, keep_half=False):\n",
    "    new_sentence = swapsies_nd(sentence, swap_prob, max_dist)\n",
    "    new_sentence = dropsies_nd(new_sentence, drop_prob, keep_half=False)\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def more(train_set, drop_prob, swap_prob, max_dist, keep_half=False):\n",
    "    new = []\n",
    "    for example in train_set:\n",
    "        ne = data.example.Example()\n",
    "        ne.text = augmentie_nd(example.text, drop_prob, swap_prob, max_dist, keep_half=keep_half)\n",
    "        ne.label = example.label\n",
    "        new.append(ne)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = more(train, 0.2, 0.2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.examples += m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Vocab\n",
      "Loaders\n"
     ]
    }
   ],
   "source": [
    "print(\"Build Vocab\")\n",
    "textField.build_vocab(train, dev, vectors=GloVe(name=\"6B\", dim=50))\n",
    "labelField.build_vocab(train, dev)\n",
    "\n",
    "#return\n",
    "print(\"Loaders\")\n",
    "trainLoader, testLoader = data.BucketIterator.splits((train, dev), shuffle=True, batch_size=64,\n",
    "                                                     sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
    "# already batched up by here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_len_fn(padding, kernel_size, dilation, stride):\n",
    "    def conv_len(length):\n",
    "        return ((length + 2 * padding - dilation * (kernel_size - 1) - 1) / stride) + 1\n",
    "    return conv_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for creating the neural network.\n",
    "class Network(tnn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        # config\n",
    "        self.hiddenSize = 100\n",
    "        self.bidirectional = True\n",
    "        \n",
    "        self.padding = 5\n",
    "        self.kernel_size = 8\n",
    "        self.conv_formula = lambda l: ((l + 2 * self.padding - 1 * (self.kernel_size - 1) - 1) / 1) + 1\n",
    "        self.conv = tnn.Conv1d(50, 50, kernel_size=self.kernel_size, padding=self.padding)\n",
    "        self.mp_kernel_size = 4\n",
    "        self.mp = tnn.MaxPool1d(self.mp_kernel_size)\n",
    "        self.mp_formula = lambda l: ((l + 2 * 0 - 1 * (self.mp_kernel_size - 1) - 1) / 1) + 1\n",
    "        self.lstm = tnn.LSTM(50,\n",
    "                            self.hiddenSize,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=self.bidirectional,\n",
    "                            num_layers=1,\n",
    "                            dropout=0)\n",
    "        self.fc1 = tnn.Linear(self.hiddenSize*(self.bidirectional+1), 100)\n",
    "        self.drop = tnn.Dropout(0.5)\n",
    "        self.relu = tnn.ReLU()\n",
    "        self.fc2 = tnn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, input, length):\n",
    "        \"\"\"\n",
    "        DO NOT MODIFY FUNCTION SIGNATURE\n",
    "        Create the forward pass through the network.\n",
    "        \"\"\"\n",
    "        print(input.shape)\n",
    "        X = input.permute((0,2,1))\n",
    "        X = self.mp(self.conv(X))\n",
    "        X = X.permute((0,2,1))\n",
    "        print(X.shape)\n",
    "        new_lengths = self.mp_formula(self.conv_formula(length))\n",
    "        print(length)\n",
    "        print(new_lengths)\n",
    "        return\n",
    "        packed = tnn.utils.rnn.pack_padded_sequence(X, conv_lengths, batch_first=True)\n",
    "        y, (hn, cn) = self.lstm(packed)\n",
    "        y = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        y = self.relu(self.fc1(y))\n",
    "        y = self.fc2(y)\n",
    "        return y.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def compose(*functions):\n",
    "    def compose2(f, g):\n",
    "        return lambda x: f(g(x))\n",
    "    return functools.reduce(compose2, functions, lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for creating the neural network.\n",
    "class Network(tnn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        # config\n",
    "        self.conv_hidden = 100\n",
    "        self.conv1 = tnn.Conv1d(50, self.conv_hidden, kernel_size=1, padding=5)\n",
    "        self.conv1_len = conv_len_fn(5, 1, 1, 1)\n",
    "        self.conv2 = tnn.Conv1d(self.conv_hidden, self.conv_hidden, kernel_size=2, padding=5)\n",
    "        self.conv2_len = conv_len_fn(5, 2, 1, 1)\n",
    "        self.conv3 = tnn.Conv1d(self.conv_hidden, self.conv_hidden, kernel_size=3, padding=5)\n",
    "        self.conv3_len = conv_len_fn(5, 3, 1, 1)\n",
    "        self.conv4 = tnn.Conv1d(self.conv_hidden, self.conv_hidden, kernel_size=4, padding=5)\n",
    "        self.conv4_len = conv_len_fn(5, 4, 1, 1)\n",
    "        \n",
    "        self.mp = tnn.MaxPool1d(2)\n",
    "        self.mp_len = conv_len_fn(0, 2, 1, 2)\n",
    "        \n",
    "        self.new_len = compose(self.mp_len, self.conv4_len,\n",
    "                               self.mp_len, self.conv3_len,\n",
    "                               self.mp_len, self.conv2_len,\n",
    "                               self.mp_len, self.conv1_len)\n",
    "        \n",
    "        self.mpot = tnn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        self.conv_fc1 = tnn.Linear(self.conv_hidden, 1)\n",
    "        \n",
    "        self.lstm = tnn.LSTM(100, 200, batch_first=True)\n",
    "        self.fc1 = tnn.Linear(200, 100)\n",
    "        self.fc2 = tnn.Linear(100, 1)\n",
    "        \n",
    "        self.do = tnn.Dropout(0.5)\n",
    "        self.relu = tnn.ReLU()\n",
    "        \n",
    "        self.ffc = tnn.Linear(2, 1)\n",
    "        \n",
    "    def forward(self, input, length):\n",
    "        \"\"\"\n",
    "        DO NOT MODIFY FUNCTION SIGNATURE\n",
    "        Create the forward pass through the network.\n",
    "        \"\"\"\n",
    "        print(input.shape)\n",
    "        C = input.permute((0,2,1)) #1,2,0?\n",
    "        print(C.shape)\n",
    "        C = C.permute((0,2,1))\n",
    "        print(torch.cat((C, C), dim=1).shape)\n",
    "        C = self.mp(self.relu(self.conv1(C)))\n",
    "        C = C.permute((0,2,1))\n",
    "        return\n",
    "        nl = self.mp_len(self.conv1_len(length))\n",
    "        packed = tnn.utils.rnn.pack_padded_sequence(C, nl, batch_first=True)\n",
    "        _, (L, _) = self.lstm(packed)\n",
    "        L = self.relu(self.fc1(L))\n",
    "        L = self.fc2(L)\n",
    "        return L.squeeze()\n",
    "        L = L.reshape((L.shape[1],1))\n",
    "        \n",
    "        X = torch.cat((C, L), dim=1)\n",
    "        X = self.ffc(X)\n",
    "        return X.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to train\n",
      "torch.Size([64, 165, 50])\n",
      "torch.Size([64, 50, 165])\n",
      "torch.Size([64, 330, 50])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size 100 50 1, expected input[64, 165, 50] to have 50 channels, but got 165 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-eed818a58b10>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# Forward pass through the network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-175-2d34f57406bf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, length)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    201\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 202\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size 100 50 1, expected input[64, 165, 50] to have 50 channels, but got 165 channels instead"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6248"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aileen', 'gonsalves,', 'my', 'girlfriend,', 'is', 'in', 'this', 'film', 'playing', 'a', 'secretary', 'at', 'the', 'main', \"character's\", 'bank.', 'she', 'has', 'a', 'lovely', 'scene', 'with', 'roshan', 'seth', 'in', 'a', 'restaurant.', \"there's\", 'more', 'information', 'on', 'her', 'website', 'at', '>having', 'stated', 'my', 'personal', 'interest', 'in', 'the', 'film,', 'i', 'have', 'to', 'say', 'that', 'i', 'think', 'it', 'is', 'a', 'beautiful', 'movie', '-', 'moving,', 'funny', 'and', 'beautifully', 'filmed.']\n"
     ]
    }
   ],
   "source": [
    "print(train.examples[10].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aileen', 'gonsalves', 'my', 'girlfriend', 'is', 'in', 'this', 'film', 'playing', 'a', 'secretary', 'at', 'the', 'main', 'characters', 'bank', 'she', 'has', 'a', 'lovely', 'scene', 'with', 'roshan', 'seth', 'in', 'a', 'restaurant', 'theres', 'more', 'information', 'on', 'her', 'website', 'at', 'having', 'stated', 'my', 'personal', 'interest', 'in', 'the', 'film', 'i', 'have', 'to', 'say', 'that', 'i', 'think', 'it', 'is', 'a', 'beautiful', 'movie', 'moving', 'funny', 'and', 'beautifully', 'filmed']\n"
     ]
    }
   ],
   "source": [
    "only_alphabet = lambda w: re.sub('[^a-z]', '', w)\n",
    "not_empty = lambda w: len(w) != 0\n",
    "print(list(filter(not_empty, map(only_alphabet, train.examples[10].text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
